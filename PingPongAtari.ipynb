{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba222b3",
   "metadata": {},
   "source": [
    "Name: Mahendran Jinachandran \n",
    "\n",
    "Student ID: 24088951"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2890f85a",
   "metadata": {},
   "source": [
    "# Section 1: \n",
    "Why Reinforcement Learning is the ML paradigm of choice for this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4340c4d",
   "metadata": {},
   "source": [
    "- I have chosen \"Ping Pong\" game to work on this task. Reinforcement Learning(RL) is the most suitable for this task because it requires control and making decisions. Unlike supervised learning methods that rely on large datasets, RL does not require any datasets it rather relies on experiences just like we humans do.\n",
    "\n",
    "- In the game of \"Ping Pong\", the agent (which is a software in this case) should learn how to move up or down to bounce the ball and score points to win the match. This is one of the examples of \"Sequential Decision-Making\" where each move impacts the game's results. \n",
    "\n",
    "- The one thing which separates RL from the other ML techniques is that the agent starts with no experience of the game prior. It learns through trial and error, explores different actions, receives feedback through those actions in from of rewards or penalties and eventually learns to make better choices to maximize its score. This is one of the techniques, which makes RL a very powerful approach for Atari games and many more, in which the rules are fixed but the strategies will be diffirent which much be learnt. \n",
    "\n",
    "Just For Fun: Hopefully, the game learns to make much better decisions than I do in real-life. Let's dive into the game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1412d",
   "metadata": {},
   "source": [
    "# Section 2:\n",
    "## The Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a117aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f49ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d8aea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for available GPU devices and use them\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth for GPUs (avoid memory allocation errors)\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76b677",
   "metadata": {},
   "source": [
    "a. The Atari game selected is *Pong-v5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425a54e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "#env = gym.make(\"ALE/Pong-v5\", render_mode=\"human\") # render_mode = 'human' for rendering\n",
    "env = gym.make(\"ALE/Pacman-v5\", render_mode=\"human\") # render_mode = 'human' for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c326e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.metadata['render_fps'] = 60 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd738d",
   "metadata": {},
   "source": [
    "b. Inputs received from Gym Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b984f1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (250, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "print(\"Observation shape:\", observation[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0247e20",
   "metadata": {},
   "source": [
    "c. Control settings for the JoyStick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73769dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible actions: 5\n",
      "Available actions are: \n",
      "['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN']\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "print(\"Number of possible actions:\", num_actions)\n",
    "print(\"Available actions are: \")\n",
    "print(env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20832cc7",
   "metadata": {},
   "source": [
    "# Section 3: DQN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9ade1",
   "metadata": {},
   "source": [
    "a. Capture and Preprocessing of the Data (1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41aaca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame_color(frame):\n",
    "    \"\"\"\n",
    "    Resize a raw RGB frame to 84x84 while keeping 3 color channels.\n",
    "    \"\"\"\n",
    "    return cv2.resize(frame, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def initialize_frame_stack(preprocessed_frame, stack_size=4):\n",
    "    \"\"\"\n",
    "    Initialize a deque of stacked frames with the same frame repeated.\n",
    "    \"\"\"\n",
    "    return deque([preprocessed_frame for _ in range(stack_size)], maxlen=stack_size)\n",
    "\n",
    "def stack_frames(stacked_frames, new_frame, is_new_episode, stack_size=4):\n",
    "    \"\"\"\n",
    "    Initialize the frame stack if it's a new episode, otherwise\n",
    "    append the new frame to the existing stack.\n",
    "    \"\"\"\n",
    "    if is_new_episode:\n",
    "        stacked_frames = initialize_frame_stack(new_frame, stack_size)\n",
    "    else:\n",
    "        stacked_frames.append(new_frame)\n",
    "        \n",
    "    # Concatenate along the channel dimension: (84, 84, 12) for 4 RGB frames\n",
    "    stacked_state = np.concatenate(stacked_frames, axis=2)\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54966cb",
   "metadata": {},
   "source": [
    "b. The Network Structure (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "754f1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dqn_model(input_shape, num_actions):\n",
    "    \"\"\"\n",
    "    Create a Convolutional Neural Network for DQN.\n",
    "    Input shape = (84, 84, 12) for 4 stacked color frames.\n",
    "    Output = Q-value for each action.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "\n",
    "        layers.Conv2D(32, (8, 8), strides=2, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.Conv2D(64, (4, 4), strides=2, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.Conv2D(64, (3, 3), strides=1, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.Flatten(),\n",
    "\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),  # Dropout to avoid overfitting\n",
    "        \n",
    "        layers.Dense(num_actions) \n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84d81a4",
   "metadata": {},
   "source": [
    "c. Q-Learning Update (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9824ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(main_model, target_model, optimizer, batch, gamma=0.99):\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "    # Convert to tensors\n",
    "    states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "    next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "    rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "    dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "    # Train step\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Q(s, a) using the main model\n",
    "        q_values = main_model(states)\n",
    "        q_action = tf.reduce_sum(q_values * tf.one_hot(actions, main_model.output_shape[-1]), axis=1)\n",
    "\n",
    "        # Q(s', a') using the target model (no gradient)\n",
    "        next_q_values = target_model(next_states)\n",
    "        max_next_q = tf.reduce_max(next_q_values, axis=1)\n",
    "        target_q = rewards + gamma * max_next_q * (1.0 - dones)\n",
    "\n",
    "        # Loss between predicted Q and target Q\n",
    "        loss = tf.keras.losses.MSE(target_q, q_action)\n",
    "\n",
    "    # Apply gradients\n",
    "    grads = tape.gradient(loss, main_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, main_model.trainable_variables))\n",
    "    loss = loss.numpy()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d572b",
   "metadata": {},
   "source": [
    "d. Other Important Concepts (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c781c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "GAMMA = 0.99 # Discount factor\n",
    "EPSILON = 1.0   # Exploration rate\n",
    "EPSILON_MIN = 0.1 # Minimum exploration rate\n",
    "EPSILON_DECAY = 0.995 # Decay rate\n",
    "LEARNING_RATE = 0.00025 # Learning rate\n",
    "REPLAY_BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 32 # Batch size for training\n",
    "TARGET_UPDATE_FREQ = 1000 # Frequency to update target model\n",
    "STACK_SIZE =  4 # Number of frames to stack\n",
    "TOTAL_EPISODES = 500 # Total episodes to train\n",
    "MAX_STEPS = 5000 # Max steps per episode\n",
    "input_shape = (84, 84, 3 * STACK_SIZE) # 4 stacked frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1402b32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 17:38:04.514232: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-04-13 17:38:04.514370: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-04-13 17:38:04.514374: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-04-13 17:38:04.514977: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-13 17:38:04.515233: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Models and optimizer\n",
    "main_model = create_dqn_model(input_shape, num_actions)\n",
    "target_model = create_dqn_model(input_shape, num_actions)\n",
    "target_model.set_weights(main_model.get_weights())\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Replay buffer\n",
    "replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "\n",
    "# Frame stack\n",
    "stacked_frames = deque(maxlen=STACK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec4fb8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(stacked_state):\n",
    "    # Epsilon-greedy action\n",
    "    if np.random.rand() < EPSILON:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        q_values = main_model(np.expand_dims(stacked_state, axis=0), training=False)\n",
    "        action = np.argmax(q_values.numpy())\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0142162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c458138340774888bfbca796a895ff8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Episodes:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a5eb1278e04a6f895ae3b901683953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode 1:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ Episode 1/500 - Reward: 11.0 - Epsilon: 0.9950 - ‚è±Ô∏è Time: 52.07s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6caca4fa9c542edac0db2bf6a0c4f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode 2:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ Episode 2/500 - Reward: 12.0 - Epsilon: 0.9900 - ‚è±Ô∏è Time: 77.00s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9514ffa27a04fa0bdc05025df2dd904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode 3:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1000] üîÑ Target network updated.\n",
      "üéÆ Episode 3/500 - Reward: 23.0 - Epsilon: 0.9851 - ‚è±Ô∏è Time: 80.91s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6f0c9315b84c9589ac725b695c8069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode 4:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ Episode 4/500 - Reward: 6.0 - Epsilon: 0.9801 - ‚è±Ô∏è Time: 49.78s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb28dbcbe3a4106849f939b94fd1803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode 5:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2000] üîÑ Target network updated.\n",
      "üéÆ Episode 5/500 - Reward: 24.0 - Epsilon: 0.9752 - ‚è±Ô∏è Time: 77.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458d8d74b08a41479b1268bd707e3c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode 6:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ Episode 6/500 - Reward: 19.0 - Epsilon: 0.9704 - ‚è±Ô∏è Time: 112.50s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2026ce5d95f448798924844dc237c1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode 7:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3000] üîÑ Target network updated.\n",
      "üéÆ Episode 7/500 - Reward: 17.0 - Epsilon: 0.9655 - ‚è±Ô∏è Time: 64.72s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de47245e844044b9a16a839bf292071c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode 8:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ Episode 8/500 - Reward: 32.0 - Epsilon: 0.9607 - ‚è±Ô∏è Time: 102.21s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf2de4e25ee438a9a24dbc51b774ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode 9:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4000] üîÑ Target network updated.\n",
      "üéÆ Episode 9/500 - Reward: 22.0 - Epsilon: 0.9559 - ‚è±Ô∏è Time: 71.33s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732573d88b484472aa0d7cbdea4751da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode 10:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ Episode 10/500 - Reward: 9.0 - Epsilon: 0.9511 - ‚è±Ô∏è Time: 53.90s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542991fdd0944b7f8fcc6be50e10e980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode 11:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/17/js2qkwqd7cqdm6wdqkfsnpm00000gn/T/ipykernel_29979/469461664.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Train enough samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Update target network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep_count\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mTARGET_UPDATE_FREQ\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/17/js2qkwqd7cqdm6wdqkfsnpm00000gn/T/ipykernel_29979/177834520.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(main_model, target_model, optimizer, batch, gamma)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Train step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Q(s, a) using the main model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mq_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Q(s', a') using the target model (no gradient)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnext_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/CodeToGreatness/NeuroForge/ReinforcementLearning/myenv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/CodeToGreatness/NeuroForge/ReinforcementLearning/myenv/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/CodeToGreatness/NeuroForge/ReinforcementLearning/myenv/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(indices, depth, on_value, off_value, axis, dtype, name)\u001b[0m\n\u001b[1;32m   4287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0moff_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4288\u001b[0m       raise TypeError(\"dtype {0} of on_value does not match \"\n\u001b[1;32m   4289\u001b[0m                       \"dtype {1} of off_value\".format(on_dtype, off_dtype))\n\u001b[1;32m   4290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4291\u001b[0;31m     return gen_array_ops.one_hot(indices, depth, on_value, off_value, axis,\n\u001b[0m\u001b[1;32m   4292\u001b[0m                                  name)\n",
      "\u001b[0;32m~/Documents/CodeToGreatness/NeuroForge/ReinforcementLearning/myenv/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(indices, depth, on_value, off_value, axis, name)\u001b[0m\n\u001b[1;32m   6431\u001b[0m         axis)\n\u001b[1;32m   6432\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6433\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6434\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6435\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6436\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6438\u001b[0m       return one_hot_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "step_count = 0\n",
    "\n",
    "for episode in tqdm(range(TOTAL_EPISODES), desc=\"Training Episodes\"):\n",
    "    start_time = time.time()\n",
    "\n",
    "    state, info = env.reset()\n",
    "    preprocessed_frame = preprocess_frame_color(state)\n",
    "    stacked_state, stacked_frames = stack_frames(stacked_frames, preprocessed_frame, True)\n",
    "\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    for step in tqdm(range(MAX_STEPS), desc=f\"Episode {episode+1}\", leave=False):\n",
    "        step_count += 1\n",
    "        if np.random.rand() < EPSILON:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_values = main_model(np.expand_dims(stacked_state, axis=0), training=False)\n",
    "            action = np.argmax(q_values.numpy())\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Process next frame\n",
    "        preprocessed_next = preprocess_frame_color(next_state)\n",
    "        next_stacked_state, stacked_frames = stack_frames(stacked_frames, preprocessed_next, False)\n",
    "\n",
    "        # Store experience\n",
    "        replay_buffer.add((stacked_state, action, reward, next_stacked_state, float(done)))\n",
    "\n",
    "        stacked_state = next_stacked_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Train enough samples\n",
    "        if len(replay_buffer) > BATCH_SIZE:\n",
    "            batch = replay_buffer.sample(BATCH_SIZE)\n",
    "            loss = update_model(main_model, target_model, optimizer, batch, gamma=GAMMA)\n",
    "\n",
    "        # Update target network\n",
    "        if step_count % TARGET_UPDATE_FREQ == 0:\n",
    "            target_model.set_weights(main_model.get_weights())\n",
    "            print(f\"[Step {step_count}] üîÑ Target network updated.\")\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    EPSILON = max(EPSILON_MIN, EPSILON * EPSILON_DECAY)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    tqdm.write(f\"üéÆ Episode {episode + 1}/{TOTAL_EPISODES} - Reward: {total_reward} - Epsilon: {EPSILON:.4f} - ‚è±Ô∏è Time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3088e814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
