{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba222b3",
   "metadata": {},
   "source": [
    "Name: Mahendran Jinachandran \n",
    "\n",
    "Student ID: 24088951"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2890f85a",
   "metadata": {},
   "source": [
    "# Section 1: \n",
    "Why Reinforcement Learning is the ML paradigm of choice for this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4340c4d",
   "metadata": {},
   "source": [
    "- I have chosen \"Ping Pong\" game to work on this task. Reinforcement Learning(RL) is the most suitable for this task because it requires control and making decisions. Unlike supervised learning methods that rely on large datasets, RL does not require any datasets it rather relies on experiences just like we humans do.\n",
    "\n",
    "- In the game of \"Ping Pong\", the agent (which is a software in this case) should learn how to move up or down to bounce the ball and score points to win the match. This is one of the examples of \"Sequential Decision-Making\" where each move impacts the game's results. \n",
    "\n",
    "- The one thing which separates RL from the other ML techniques is that the agent starts with no experience of the game prior. It learns through trial and error, explores different actions, receives feedback through those actions in from of rewards or penalties and eventually learns to make better choices to maximize its score. This is one of the techniques, which makes RL a very powerful approach for Atari games and many more, in which the rules are fixed but the strategies will be diffirent which much be learnt. \n",
    "\n",
    "Just For Fun: Hopefully, the game learns to make much better decisions than I do in real-life. Let's dive into the game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1412d",
   "metadata": {},
   "source": [
    "# Section 2:\n",
    "## The Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a117aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gym\n",
    "import cv2 \n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f49ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8aea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for available GPU devices and use them\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76b677",
   "metadata": {},
   "source": [
    "a. The Atari game selected is *Pong-v5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make(\"ALE/Pong-v5\", render_mode=\"human\") # render_mode = 'human' for rendering\n",
    "env = gym.make(\"ALE/Pacman-v5\", render_mode=\"rgb_array\") # render_mode = 'human' for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c326e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.metadata['render_fps'] = 60 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd738d",
   "metadata": {},
   "source": [
    "b. Inputs received from Gym Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b984f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "print(\"Observation shape:\", observation[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0247e20",
   "metadata": {},
   "source": [
    "c. Control settings for the JoyStick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73769dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "print(\"Number of possible actions:\", num_actions)\n",
    "print(\"Available actions are: \")\n",
    "print(env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20832cc7",
   "metadata": {},
   "source": [
    "# Section 3: DQN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9ade1",
   "metadata": {},
   "source": [
    "a. Capture and Preprocessing of the Data (1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aaca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame_color(frame):\n",
    "    \"\"\"\n",
    "    Resize a raw RGB frame to 84x84 while keeping 3 color channels.\n",
    "    \"\"\"\n",
    "    return cv2.resize(frame, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def initialize_frame_stack(preprocessed_frame, stack_size=4):\n",
    "    \"\"\"\n",
    "    Initialize a deque of stacked frames with the same frame repeated.\n",
    "    \"\"\"\n",
    "    total_frames = [preprocessed_frame for _ in range(stack_size)]\n",
    "    return deque(total_frames, maxlen=stack_size)\n",
    "\n",
    "def stack_frames(stacked_frames, new_frame, is_new_episode, stack_size=4):\n",
    "    \"\"\"\n",
    "    Initialize the frame stack if it's a new episode, otherwise\n",
    "    append the new frame to the existing stack.\n",
    "    \"\"\"\n",
    "    if is_new_episode:\n",
    "        stacked_frames = initialize_frame_stack(new_frame, stack_size)\n",
    "    else:\n",
    "        stacked_frames.append(new_frame)\n",
    "        \n",
    "    # Concatenate along the channel dimension: (84, 84, 12) for 4 RGB frames\n",
    "    stacked_state = np.concatenate(stacked_frames, axis=2)\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54966cb",
   "metadata": {},
   "source": [
    "b. The Network Structure (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dqn_model(input_shape, num_actions):\n",
    "    \"\"\"\n",
    "    Create a Convolutional Neural Network for DQN.\n",
    "    Input shape = (84, 84, 12) for 4 stacked color frames.\n",
    "    Output = Q-value for each action.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "\n",
    "        layers.Conv2D(32, (8, 8), strides=2, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.Conv2D(64, (4, 4), strides=2, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.Conv2D(64, (3, 3), strides=1, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.Flatten(),\n",
    "\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),  # Dropout to avoid overfitting\n",
    "        \n",
    "        layers.Dense(num_actions) \n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84d81a4",
   "metadata": {},
   "source": [
    "c. Q-Learning Update (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9824ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(main_model, target_model, optimizer, batch, gamma=0.99):\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "    # Convert to tensors\n",
    "    states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "    next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "    rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "    dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "    # Train step\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Q(s, a) using the main model\n",
    "        q_values = main_model(states)\n",
    "        q_action = tf.reduce_sum(q_values * tf.one_hot(actions, main_model.output_shape[-1]), axis=1)\n",
    "\n",
    "        # Q(s', a') using the target model (no gradient)\n",
    "        next_q_values = target_model(next_states)\n",
    "        max_next_q = tf.reduce_max(next_q_values, axis=1)\n",
    "        target_q = rewards + gamma * max_next_q * (1.0 - dones)\n",
    "\n",
    "        # Loss between predicted Q and target Q\n",
    "        loss = tf.keras.losses.MSE(target_q, q_action)\n",
    "\n",
    "    # Apply gradients\n",
    "    grads = tape.gradient(loss, main_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, main_model.trainable_variables))\n",
    "    loss = loss.numpy()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d572b",
   "metadata": {},
   "source": [
    "d. Other Important Concepts (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c781c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "GAMMA = 0.99 # Discount factor\n",
    "EPSILON = 1.0   # Exploration rate\n",
    "EPSILON_MIN = 0.1 # Minimum exploration rate\n",
    "EPSILON_DECAY = 0.995 # Decay rate\n",
    "LEARNING_RATE = 0.00025 # Learning rate\n",
    "REPLAY_BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 32 # Batch size for training\n",
    "TARGET_UPDATE_FREQ = 1000 # Frequency to update target model\n",
    "STACK_SIZE =  4 # Number of frames to stack\n",
    "TOTAL_EPISODES = 500 # Total episodes to train\n",
    "MAX_STEPS = 5000 # Max steps per episode\n",
    "input_shape = (84, 84, 3 * STACK_SIZE) # 4 stacked frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1402b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models and optimizer\n",
    "dqn_main_model = create_dqn_model(input_shape, num_actions)\n",
    "dqn_target_model = create_dqn_model(input_shape, num_actions)\n",
    "dqn_target_model.set_weights(dqn_main_model.get_weights())\n",
    "dqn_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Replay buffer\n",
    "dqn_replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "\n",
    "# Frame stack\n",
    "dqn_stacked_frames = deque(maxlen=STACK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4fb8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(stacked_state, main_model):\n",
    "    # Epsilon-greedy action\n",
    "    if np.random.rand() < EPSILON:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        q_values = main_model(np.expand_dims(stacked_state, axis=0), training=False)\n",
    "        action = np.argmax(q_values.numpy())\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0142162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(dqn_type, main_model, target_model, optimizer, replay_buffer, stacked_frames):\n",
    "    training_start_time = time.time()\n",
    "    step_count = 0\n",
    "    episode_rewards = []\n",
    "    epsilons = []\n",
    "    losses = []\n",
    "\n",
    "    print(f\"Training of {dqn_type} DQN started...\")\n",
    "    for episode in tqdm(range(TOTAL_EPISODES), desc=\"Training Episodes\"):\n",
    "        start_time = time.time()\n",
    "\n",
    "        state, info = env.reset()\n",
    "        preprocessed_frame = preprocess_frame_color(state)\n",
    "        stacked_state, stacked_frames = stack_frames(stacked_frames, preprocessed_frame, True)\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        for step in tqdm(range(MAX_STEPS), desc=f\"Episode {episode+1}\", leave=False):\n",
    "            step_count += 1\n",
    "\n",
    "            action = choose_action(stacked_state, main_model)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Process next frame\n",
    "            preprocessed_next = preprocess_frame_color(next_state)\n",
    "            next_stacked_state, stacked_frames = stack_frames(stacked_frames, preprocessed_next, False)\n",
    "\n",
    "            # Store experience\n",
    "            replay_buffer.add((stacked_state, action, reward, next_stacked_state, float(done)))\n",
    "            stacked_state = next_stacked_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Train enough samples\n",
    "            if len(replay_buffer) > BATCH_SIZE:\n",
    "                batch = replay_buffer.sample(BATCH_SIZE)\n",
    "                loss = update_model(main_model, target_model, optimizer, batch, gamma=GAMMA)\n",
    "                losses.append(loss)\n",
    "\n",
    "            # Update target network\n",
    "            if step_count % TARGET_UPDATE_FREQ == 0:\n",
    "                target_model.set_weights(main_model.get_weights())\n",
    "                print(f\"[Step {step_count}] ðŸ”„ Target network updated.\")\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Decay epsilon\n",
    "        global EPSILON\n",
    "        EPSILON = max(EPSILON_MIN, EPSILON * EPSILON_DECAY)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        episode_rewards.append(total_reward)\n",
    "        epsilons.append(EPSILON)\n",
    "        tqdm.write(f\"Episode {episode + 1}/{TOTAL_EPISODES} - Reward: {total_reward} - Epsilon: {EPSILON:.4f} - Time: {elapsed:.2f}s\")\n",
    "\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    return episode_rewards, epsilons, losses, total_training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_episode_rewards, dqn_epsilons, dqn_losses, dqn_total_training_time = train_dqn(\"Q-Learning\", dqn_main_model, dqn_target_model, dqn_optimizer, dqn_replay_buffer, dqn_stacked_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d7b195",
   "metadata": {},
   "source": [
    "# Section 4: \n",
    "Results and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76445e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A generic method to display training metrics\n",
    "# such as rewards, epsilon, and loss\n",
    "def plot_training_metric(metric_values, title=\"Training Metric\", ylabel=\"Value\", xlabel=\"Episode\", label=None, color='blue'):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(metric_values, label=label if label else ylabel, color=color)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deee31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot using the generic function\n",
    "plot_training_metric(dqn_episode_rewards, title=\"DQN Training: Reward Per Episode\", ylabel=\"Reward\", label=\"Episode Reward\", color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_metric(dqn_epsilons, title=\"DQN Exploration Rate (Epsilon) Over Time\", ylabel=\"Epsilon\", label=\"Epsilon\", color=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6264e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_metric(dqn_losses, title=\"DQN Loss Over Time\", ylabel=\"Loss\", label=\"Loss\", color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62815673",
   "metadata": {},
   "source": [
    "# Section 5\n",
    "Exploration of recent developments in DQN i.e Dueling DQNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68063c37",
   "metadata": {},
   "source": [
    "Reference: Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e84d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dueling_dqn(input_shape, num_actions):\n",
    "    \"\"\"\n",
    "    TensorFlow (Keras) implementation of Dueling DQN based on the provided PyTorch model.\n",
    "    input_shape: (height, width, channels) â€” e.g., (84, 84, 1)\n",
    "    num_actions: number of possible actions in the environment\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Convolutional layers\n",
    "    x = layers.Conv2D(32, kernel_size=8, strides=4, activation='relu')(inputs)\n",
    "    x = layers.Conv2D(64, kernel_size=4, strides=2, activation='relu')(x)\n",
    "    x = layers.Conv2D(64, kernel_size=3, strides=1, activation='relu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    # Shared dense layer\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "    # Value stream\n",
    "    value = layers.Dense(1)(x)  # Output: scalar\n",
    "\n",
    "    # Advantage stream\n",
    "    advantage = layers.Dense(num_actions)(x)  # Output: one value per action\n",
    "\n",
    "    # Combine value and advantage into Q-values\n",
    "    advantage_mean = tf.reduce_mean(advantage, axis=1, keepdims=True)\n",
    "    q_values = value + (advantage - advantage_mean)\n",
    "\n",
    "    # Final model\n",
    "    model = models.Model(inputs=inputs, outputs=q_values)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a49e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models and optimizer\n",
    "dueling_dqn_main_model = create_dueling_dqn(input_shape, num_actions)\n",
    "dueling_dqn_target_model = create_dueling_dqn(input_shape, num_actions)\n",
    "dueling_dqn_target_model.set_weights(dueling_dqn_main_model.get_weights())\n",
    "dueling_dqn_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Replay buffer\n",
    "dueling_dqn_replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "\n",
    "# Frame stack\n",
    "dueling_dqn_stacked_frames = deque(maxlen=STACK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dueling_dqn_rewards, dueling_dqn_epsilons, dueling_dqn_losses, dueling_dqn_total_training_time = train_dqn(\"Dueling DQN\", dueling_dqn_main_model, dueling_dqn_target_model, dueling_dqn_optimizer, dueling_dqn_replay_buffer, dueling_dqn_stacked_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_metric(dueling_dqn_rewards, title=\"Dueling DQN Training: Reward Per Episode\", ylabel=\"Reward\", label=\"Episode Reward\", color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbee6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_metric(dueling_dqn_epsilons, title=\"Dueling DQN Exploration Rate (Epsilon) Over Time\", ylabel=\"Epsilon\", label=\"Epsilon\", color=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447423d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_metric(dueling_dqn_losses, title=\"Dueling DQN Loss Over Time\", ylabel=\"Loss\", label=\"Loss\", color=\"red\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
